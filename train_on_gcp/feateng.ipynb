{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Feature Engineering </h1>\n",
    "\n",
    "In this notebook, you will learn how to incorporate feature engineering into your pipeline.\n",
    "<ul>\n",
    "<li> Working with feature columns </li>\n",
    "<li> Adding feature crosses in TensorFlow </li>\n",
    "<li> Reading data from BigQuery </li>\n",
    "<li> Creating datasets using Dataflow </li>\n",
    "<li> Using a wide-and-deep model </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam only works in Python 2 at the moment, so we're going to switch to the Python 2 kernel. In the above menu, click the dropdown arrow and select `python2`. After that, run the following to ensure we've installed Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata: ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting apache-beam[gcp]==2.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/3d/90aa15779e884feebae4b0c26cad6f52cd4040397a94deb58dad9c8b7300/apache_beam-2.9.0-cp27-cp27mu-manylinux1_x86_64.whl (2.4MB)\n",
      "Requirement already satisfied: dill<=0.2.8.2,>=0.2.6 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (0.2.6)\n",
      "Collecting pydot<1.3,>=1.2.0 (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/f1/e61d6dfe6c1768ed2529761a68f70939e2569da043e9f15a8d84bf56cadf/pydot-1.2.4.tar.gz (132kB)\n",
      "Collecting httplib2<=0.11.3,>=0.8 (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/ce/aa4a385e3e9fd351737fd2b07edaa56e7a730448465aceda6b35086a0d9b/httplib2-0.11.3.tar.gz (215kB)\n",
      "Requirement already satisfied: pyyaml<4.0.0,>=3.12 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (3.13)\n",
      "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (2.0.0)\n",
      "Collecting typing<3.7.0,>=3.6.0; python_version < \"3.5.0\" (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/3e/29f92b7aeda5b078c86d14f550bf85cff809042e3429ace7af6193c3bc9f/typing-3.6.6-py2-none-any.whl\n",
      "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/b6/36bfb1760f6983788d916096193fc14c83cce512c7787c93380e09458c09/PyVCF-0.6.8.tar.gz\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (2.2.0)\n",
      "Requirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (0.16.0)\n",
      "Requirement already satisfied: avro<2.0.0,>=1.8.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (1.8.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (1.7)\n",
      "Collecting fastavro<0.22,>=0.21.4 (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/31/fda0abfd038684f8fd9bd42c7fb73a37f3eccabc6edf5bbfd746553e0362/fastavro-0.21.20-cp27-cp27mu-manylinux1_x86_64.whl (1.0MB)\n",
      "Requirement already satisfied: futures<4.0.0,>=3.1.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (3.2.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.8 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (1.17.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (3.6.1)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/de/07/9d4af32b643650a368ec29899bcb2a06343810b6ed5c2382d670eb35cc5b/hdfs-2.5.0.tar.gz\n",
      "Collecting pytz<=2018.4,>=2018.3 (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/83/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2/pytz-2018.4-py2.py3-none-any.whl (510kB)\n",
      "Collecting google-cloud-pubsub==0.35.4; extra == \"gcp\" (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/66/f9/bfa284399fb59a8896e0c4164b46185f61f35a90a18c67b366406ad472a6/google_cloud_pubsub-0.35.4-py2.py3-none-any.whl (93kB)\n",
      "Requirement already satisfied: proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (0.90.0)\n",
      "Collecting google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\" (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/26/abea123a1b5a2b0c1b49c0d8a2e030725f32ae0932d026f2c7a6ee32c8d3/google_apitools-0.5.24-py2-none-any.whl (129kB)\n",
      "Collecting google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\" (from apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/1b/2b95f2fefddbbece38110712c225bfb5649206f4056445653bd5ca4dc86d/google_cloud_bigquery-1.6.1-py2.py3-none-any.whl (83kB)\n",
      "Requirement already satisfied: googledatastore<7.1,>=7.0.1; python_version < \"3.0\" and extra == \"gcp\" in /usr/local/envs/py2env/lib/python2.7/site-packages (from apache-beam[gcp]==2.9.0) (7.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from pydot<1.3,>=1.2.0->apache-beam[gcp]==2.9.0) (2.3.0)\n",
      "Requirement already satisfied: funcsigs>=1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (1.0.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (5.1.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/envs/py2env/lib/python2.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (1.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/envs/py2env/lib/python2.7/site-packages (from pyvcf<0.7.0,>=0.6.8->apache-beam[gcp]==2.9.0) (40.6.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.9.0) (0.4.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.9.0) (0.2.2)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.9.0) (3.4.2)\n",
      "Requirement already satisfied: enum34>=1.0.4 in /usr/local/envs/py2env/lib/python2.7/site-packages (from grpcio<2,>=1.8->apache-beam[gcp]==2.9.0) (1.1.6)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Requirement already satisfied: requests>=2.7.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (2.18.4)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=0.1.3 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (0.1.4)\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.5.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (1.5.5)\n",
      "Collecting fasteners>=0.14 (from google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: google-resumable-media>=0.2.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (0.3.2)\n",
      "Requirement already satisfied: google-cloud-core<0.30dev,>=0.28.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-cloud-bigquery<1.7.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (0.28.1)\n",
      "Requirement already satisfied: ordereddict in /usr/local/envs/py2env/lib/python2.7/site-packages (from funcsigs>=1->mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.9.0) (1.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/py2env/lib/python2.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.9.0) (2019.3.9)\n",
      "Requirement already satisfied: google-auth<2.0.0dev,>=0.4.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (1.6.2)\n",
      "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.24,>=0.5.23; extra == \"gcp\"->apache-beam[gcp]==2.9.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/envs/py2env/lib/python2.7/site-packages (from google-auth<2.0.0dev,>=0.4.0->google-api-core[grpc]<2.0.0dev,>=0.1.3->google-cloud-pubsub==0.35.4; extra == \"gcp\"->apache-beam[gcp]==2.9.0) (2.1.0)\n",
      "Building wheels for collected packages: pydot, httplib2, pyvcf, hdfs, docopt, grpc-google-iam-v1\n",
      "  Running setup.py bdist_wheel for pydot: started\n",
      "  Running setup.py bdist_wheel for pydot: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/6a/a5/14/25541ebcdeaf97a37b6d05c7ff15f5bd20f5e91b99d313e5b4\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/1b/9c/9e/1f6fdb21dbb1fe6a99101d697f12cb8c1fa96c1587df69adba\n",
      "  Running setup.py bdist_wheel for pyvcf: started\n",
      "  Running setup.py bdist_wheel for pyvcf: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/81/91/41/3272543c0b9c61da9c525f24ee35bae6fe8f60d4858c66805d\n",
      "  Running setup.py bdist_wheel for hdfs: started\n",
      "  Running setup.py bdist_wheel for hdfs: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/e4/43/cb/5cfa9f13f579e92b6968e1f65624a6fd0e684e03deb163ce20\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: started\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
      "Successfully built pydot httplib2 pyvcf hdfs docopt grpc-google-iam-v1\n",
      "Installing collected packages: pydot, httplib2, typing, pyvcf, fastavro, docopt, hdfs, pytz, grpc-google-iam-v1, google-cloud-pubsub, monotonic, fasteners, google-apitools, google-cloud-bigquery, apache-beam\n",
      "  Found existing installation: httplib2 0.12.0\n",
      "    Uninstalling httplib2-0.12.0:\n",
      "      Successfully uninstalled httplib2-0.12.0\n",
      "  Found existing installation: pytz 2019.1\n",
      "    Uninstalling pytz-2019.1:\n",
      "      Successfully uninstalled pytz-2019.1\n",
      "  Found existing installation: google-apitools 0.5.10\n",
      "    Uninstalling google-apitools-0.5.10:\n",
      "      Successfully uninstalled google-apitools-0.5.10\n",
      "  Found existing installation: google-cloud-bigquery 0.23.0\n",
      "    Uninstalling google-cloud-bigquery-0.23.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-0.23.0\n",
      "Successfully installed apache-beam-2.9.0 docopt-0.6.2 fastavro-0.21.20 fasteners-0.14.1 google-apitools-0.5.24 google-cloud-bigquery-1.6.1 google-cloud-pubsub-0.35.4 grpc-google-iam-v1-0.11.4 hdfs-2.5.0 httplib2-0.11.3 monotonic-1.5 pydot-1.2.4 pytz-2018.4 pyvcf-0.6.8 typing-3.6.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping google-cloud-dataflow as it is not installed.\n",
      "google-cloud-bigquery 1.6.1 has requirement google-api-core<2.0.0dev,>=1.0.0, but you'll have google-api-core 0.1.4 which is incompatible.\n",
      "googledatastore 7.0.1 has requirement httplib2<0.10,>=0.9.1, but you'll have httplib2 0.11.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "conda update -y -n base -c defaults conda\n",
    "source activate py2env\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "conda install -y pytz \n",
    "pip install apache-beam[gcp]==2.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After doing a pip install, you have to ```Reset Session``` so that the new packages are picked up.  Please click on the button in the above menu.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Environment variables for project and bucket </h2>\n",
    "\n",
    "<li> Your project id is the *unique* string that identifies your project (not the project name). You can find this from the GCP Console dashboard's Home page.  My dashboard reads:  <b>Project ID:</b> cloud-training-demos </li>\n",
    "<li> Cloud training often involves saving and restoring model files. Therefore, we should <b>create a single-region bucket</b>. If you don't have a bucket already, I suggest that you create one from the GCP console (because it will dynamically check whether the bucket name you want is available) </li>\n",
    "</ol>\n",
    "<b>Change the cell below</b> to reflect your Project ID and bucket name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'qwiklabs-gcp-631b9e1cceeff549'    # CHANGE THIS\n",
    "BUCKET = 'qwiklabs-gcp-631b9e1cceeff549' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected.\n",
    "REGION = 'europe-west3-a' # Choose an available region for Cloud MLE from https://cloud.google.com/ml-engine/docs/regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8' \n",
    "\n",
    "## ensure we're using python2 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your current Cloud SDK version is: 229.0.0\n",
      "You will be upgraded to version: 242.0.0\n",
      "\n",
      "+---------------------------------------------------------+\n",
      "|            These components will be updated.            |\n",
      "+---------------------------------+------------+----------+\n",
      "|               Name              |  Version   |   Size   |\n",
      "+---------------------------------+------------+----------+\n",
      "| BigQuery Command Line Tool      |     2.0.43 |  < 1 MiB |\n",
      "| Cloud SDK Core Libraries        | 2019.04.12 | 10.1 MiB |\n",
      "| Cloud Storage Command Line Tool |       4.38 |  3.8 MiB |\n",
      "| gcloud Alpha Commands           | 2019.02.22 |  < 1 MiB |\n",
      "| gcloud Beta Commands            | 2019.02.22 |  < 1 MiB |\n",
      "| gcloud cli dependencies         | 2019.04.12 |  2.4 MiB |\n",
      "+---------------------------------+------------+----------+\n",
      "\n",
      "The following release notes are new in this upgrade.\n",
      "Please read carefully for information about new features, breaking changes,\n",
      "and bugs fixed.  The latest full release notes can be viewed at:\n",
      "  https://cloud.google.com/sdk/release_notes\n",
      "\n",
      "242.0.0 (2019-04-16)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Composer)** Deprecated support for the initdb, resetdb, and\n",
      "        upgradedb subcommands within gcloud composer environments run.\n",
      "        Execution of these subcommands can be detrimental to the Airflow\n",
      "        metadata of existing Composer environments.\n",
      "\n",
      "  Cloud Data Catalog\n",
      "      o Added the --lookup-entry flag to gcloud beta data-catalog entries\n",
      "        update to update the entry corresponding to the lookup of the given\n",
      "        resource.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.4.4\n",
      "        * Reduced lock contention for multiple writes on a single document\n",
      "        * Fix bug where no-op writes were incorrectly updating the updatedAt\n",
      "          timestamp\n",
      "      o Release Cloud Firestore Emulator version 1.4.3\n",
      "        * Fixes bug that caused parsing security rules with string literals\n",
      "          to fail\n",
      "\n",
      "  Compute Engine\n",
      "      o Updated gcloud beta compute <ssh|scp> to use IAP Tunneling by default\n",
      "        if an external interface/IP address is not available and --internal-ip\n",
      "        has not been specified.\n",
      "      o Added warning message about the maximum number of nodes that a\n",
      "        cluster can have when being created.\n",
      "      o Added the --deprecate-in and --deprecate-on flags to gcloud compute\n",
      "        images deprecate to set informational deprecate times to images.\n",
      "\n",
      "  Identity and Access Management\n",
      "      o Added the --description flag to gcloud beta iam service-accounts\n",
      "        <create|update>.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Renamed --size flag of gcloud container clusters resize to\n",
      "        --num-nodes. --size retained as an alias.\n",
      "      o Disabled node auto-repair and node auto-upgrade by default when\n",
      "    --enable-kubernetes-alpha flag is used to create clusters with Kubernetes\n",
      "    alpha features enabled. Users may now create alpha clusters without\n",
      "    specifying --no-enable-autorepair or --no-enable-autoupgrade flags.\n",
      "    However, for creating new node pools in an existing alpha cluster, these\n",
      "    two flags may still be required.\n",
      "\n",
      "  Miscellaneous\n",
      "      o Fixed a bug in gcloud config config-helper which could result in old\n",
      "        identity tokens when --force-auth-refresh was not present.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "241.0.0 (2019-04-03)\n",
      "  Breaking Changes\n",
      "      o **(Cloud Functions)** Modified gcloud functions deploy such that the\n",
      "        --runtime flag needs to be set when deploying a new function.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Java SDK to version 1.9.73. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "      o Updated the Python SDK to version 1.9.85. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "  BigQuery\n",
      "      o Added DML/DDL query results that display the number of affected rows\n",
      "        (for DML) and performed operation and target name (for DDL).\n",
      "\n",
      "  Cloud Asset Inventory\n",
      "      o Promoted gcloud asset command group to GA\n",
      "\n",
      "  Cloud Bigtable\n",
      "      o Promoted the following to GA:\n",
      "        * gcloud bigtable app-profiles command group\n",
      "        * gcloud bigtable clusters create\n",
      "        * gcloud bigtable clusters update\n",
      "        * gcloud bigtable clusters delete\n",
      "\n",
      "  Cloud Build\n",
      "      o Added --network=cloudbuild to gcloud builds submit --tag invocations\n",
      "        of docker build. This enables access to metadata during Dockerfile RUN\n",
      "        operations at build time.\n",
      "\n",
      "  Cloud Composer\n",
      "      o Added three new flags to gcloud beta composer environments create to\n",
      "        support Private IP Composer environments:\n",
      "        * --enable-private-environment\n",
      "        * --enable-private-endpoint\n",
      "        * --master-ipv4-cidr\n",
      "      o Added gcloud beta composer environments list-upgrades to list all\n",
      "        image version upgrades that are supported for a specified environment.\n",
      "      o Added two mutually exclusive flags to gcloud beta composer\n",
      "        environments update to allow for in-place environment upgrades:\n",
      "        * --airflow-version\n",
      "        * --image-version\n",
      "\n",
      "  Cloud DNS\n",
      "      o Added support for DNS peering in gcloud beta dns managed-zones.\n",
      "      o Added --enable-logging flag to gcloud beta dns policies to enable\n",
      "        query logging.\n",
      "\n",
      "  Cloud Data Catalog\n",
      "      o Added the gcloud beta data-catalog entries command group, which\n",
      "        provides lookup, describe, and schema update functionality for Cloud\n",
      "        Data Catalog entries.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.4.2\n",
      "        * Adds implementations for BeginTransaction and Rollback\n",
      "\n",
      "  Cloud Functions\n",
      "      o Added --service-account flag to gcloud functions deploy.\n",
      "      o Added --vpc-connector flag to gcloud beta functions deploy.\n",
      "\n",
      "  Cloud Memorystore\n",
      "      o Added gcloud redis instances failover which provides the ability to\n",
      "        failover a standard tier Cloud Memorystore for Redis instance from the\n",
      "        master node to its replica.\n",
      "      o Added --redis-version flag to gcloud beta redis instances create to\n",
      "        enable the specification of a preferred Redis version compatibility;\n",
      "        this can be either redis_3_2 or redis_4_0.\n",
      "      o Modified the --update-redis-config flag of gcloud redis instances\n",
      "        update to accept three additional parameters for Redis 4.0 compatible\n",
      "        instances: activedefrag, lfu-decay-time, lfu-log-factor.\n",
      "\n",
      "  Cloud PubSub\n",
      "      o Added optional flags --push-auth-service-account and\n",
      "        --push-auth-token-audience for defining an authenticated push\n",
      "        subscription in gcloud beta pubsub subscriptions\n",
      "        <create|update|modify-push-config>.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.38.\n",
      "\n",
      "  Cloud Video Intelligence API\n",
      "      o Added gcloud beta ml video\n",
      "        transcribe-speech|detect-text|detect-object commands.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --logging-aggregation-interval, --logging-flow-sampling, and\n",
      "        --logging-metadata flags of gcloud compute networks subnets\n",
      "        <create|update> to beta.\n",
      "      o Modified gcloud compute networks subnets update to support specifying\n",
      "        --logging-aggregation-interval, --logging-flow-sampling, and\n",
      "        --logging-metadata flags in a single call.\n",
      "      o Promoted gcloud compute reservations command group to beta.\n",
      "      o Promoted --reservation and --reservation-affinity of gcloud compute\n",
      "        instance-templates create to beta.\n",
      "      o Promoted --reservation and --reservations-from-file of gcloud compute\n",
      "        commitments create to beta.\n",
      "      o Promoted gcloud compute commitments update-reservations to beta.\n",
      "      o Promoted 100G interconnect link type support for gcloud compute\n",
      "        interconnects create to beta.\n",
      "      o Deprecated the creation of new legacy network.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Promoted --num-flaky-test-attempts flag of gcloud firebase test\n",
      "        <android|ios> run to GA. This flag specifies how many times to rerun\n",
      "        any failed executions.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --security-group flag of gcloud container clusters\n",
      "        create to beta. The flag enables support for Google Groups in\n",
      "        Kubernetes RBAC rules.\n",
      "      o Added the --enable-intra-node-visibility flag to gcloud beta\n",
      "        container clusters create.\n",
      "      o Promoted the --enable-tpu flag and the --tpu-ipv4-cidr flag of gcloud\n",
      "        container clusters create to GA. The flags enables support for using\n",
      "        Cloud TPU in Google Kubernetes Engine clusters.\n",
      "      o Changed the default output formatting for the gcloud beta container\n",
      "        binauthz attestations list command.\n",
      "      o Google Kubernetes Engine kubectl is updated to 1.11.9. Addresses\n",
      "        security vulnerability: CVE-2019-1002101.\n",
      "      o Updated extra Google Kubernetes Engine kubectl versions:\n",
      "        * kubectl.1.11 (patch 1.11.9)\n",
      "        * kubectl.1.12 (patch 1.12.7)\n",
      "        * kubectl.1.13 (patch 1.13.5)\n",
      "        * kubectl.1.14 (patch 1.14.0)\n",
      "      o Removed extra Google Kubernetes Engine kubectl versions, since these\n",
      "        versions are vulnerable.\n",
      "        * kubectl.1.9\n",
      "        * kubectl.1.10\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "240.0.0 (2019-03-26)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Enabled node auto-upgrade by default for\n",
      "        clusters and node-pools created with gcloud beta container\n",
      "        <clusters|node-pools> create. To disable manually, **(Kubernetes\n",
      "        Engine)** use the --no-enable-autoupgrade flag.\n",
      "\n",
      "  Cloud SDK\n",
      "      o Added the --impersonate-service-account flag to gcloud.\n",
      "\n",
      "  App Engine\n",
      "      o Fixed a bug which could cause gcloud to incorrectly print http URLs\n",
      "        for services, when https URLs would be more appropriate.\n",
      "      o Removed 32MB file size limit for second generation runtimes.\n",
      "\n",
      "  Cloud DNS\n",
      "      o Promoted private zones of the gcloud dns managed-zones command group\n",
      "        to GA. Use the --visibility and --networks flags to configure zone\n",
      "        visibility.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added gcloud beta dataproc <jobs submit|workflow-templates add-job>\n",
      "        presto to enable submitting Presto jobs to a Dataproc cluster and\n",
      "        adding Presto jobs to workflow templates, respectively.\n",
      "      o Added --enable-component-gateway flag to gcloud beta dataproc\n",
      "        clusters create and gcloud beta dataproc workflow-templates\n",
      "        set-managed-cluster.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --failover-ratio, --drop-traffic-if-unhealthy, and\n",
      "        --connection-drain-on-failover flags of gcloud compute backend-services\n",
      "        <create|update> to beta.\n",
      "      o Promoted --failover flag of gcloud compute backend-services\n",
      "        <add_backend|update_backend> to beta.\n",
      "      o Promoted gcloud compute instance-groups managed wait-until command to\n",
      "        beta.\n",
      "      o Promoted --region flag of gcloud compute disks and related commands\n",
      "        to GA.\n",
      "      o Added gcloud beta compute external-vpn-gateway command group to\n",
      "        enable reading and manipulating of Compute Engine external VPN\n",
      "        gateways.\n",
      "      o Added gcloud beta compute vpn-gateway command group to enable reading\n",
      "        and manipulating of Compute Engine VPN gateways.\n",
      "      o Updated gcloud beta compute vpn-tunnel command to enable the creation\n",
      "        of HA VPN tunnels.\n",
      "\n",
      "  Identity and Access Management\n",
      "      o Added a new column, DISABLED, to the return table of gcloud iam\n",
      "        service-account list, which displays the state of the service account\n",
      "        listed.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added the --release-channel flag to gcloud alpha container clusters\n",
      "        create for subscribing a cluster to a release channel.\n",
      "      o Promoted --default-max-pods-per-node flag of gcloud container\n",
      "        clusters create from Beta to GA.\n",
      "      o Promoted --max-pods-per-node flag of gcloud container node-pools\n",
      "        create from Beta to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "239.0.0 (2019-03-19)\n",
      "  Cloud Dataproc\n",
      "      o Added gcloud beta dataproc autoscaling-policies command group for\n",
      "        managing Cloud Dataproc autoscaling policies. For more information,\n",
      "        see:\n",
      "        https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling\n",
      "      o Added --autoscaling-policy flag to gcloud beta dataproc clusters\n",
      "        <create|update> and gcloud beta dataproc workflow-templates\n",
      "        set-managed-cluster to support enabling and disabling autoscaling on\n",
      "        Cloud Dataproc clusters with autoscaling policies.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.4.1\n",
      "        * Fixes bug where an empty CommitRequest was trigger exceptions\n",
      "\n",
      "  Cloud Resource Manager\n",
      "      o Promoted resource-manager folders command group to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted --bandwidth flag of gcloud compute interconnects attachments\n",
      "        dedicated <create|update> to GA.\n",
      "      o Promoted load_balancing_scheme enum value INTERNAL_SELF_MANAGED to\n",
      "        beta in 'gcloud compute forwarding-rules create' and 'gcloud compute\n",
      "        backend-services create'.\n",
      "      o Promoted gcloud compute instances get-shielded-identity to GA.\n",
      "      o Promoted the --shielded-vtpm, --shielded-secure-boot, and\n",
      "        --shielded-integrity-monitoring flags of gcloud compute instances\n",
      "        create to GA.\n",
      "      o Promoted the --shielded-vtpm, --shielded-secure-boot, and\n",
      "        --shielded-integrity-monitoring flags of gcloud compute\n",
      "        instance-templates create to GA.\n",
      "      o Promoted the --shielded-vtpm, --shielded-secure-boot,\n",
      "        --shielded-integrity-monitoring, and --shielded-learn-integrity-policy\n",
      "        flags of gcloud compute instances update to GA.\n",
      "      o Removed the deprecated --shielded-vm-vtpm, --shielded-vm-secure-boot,\n",
      "        and --shielded-vm-integrity-monitoring flags of gcloud compute\n",
      "        instances create from alpha and beta.\n",
      "      o Removed the deprecated --shielded-vm-vtpm, --shielded-vm-secure-boot,\n",
      "        and --shielded-vm-integrity-monitoring flags of gcloud compute\n",
      "        instance-templates create from alpha and beta.\n",
      "      o Removed the deprecated --shielded-vm-vtpm, --shielded-vm-secure-boot,\n",
      "        --shielded-vm-integrity-monitoring, and\n",
      "        --shielded-vm-learn-integrity-policy flags of gcloud compute instances\n",
      "        update from alpha and beta.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Update Google Container Engine's kubectl so that it now first\n",
      "        attempts\n",
      "    to match the API server version. If successful, the kubectl will delegate\n",
      "    to the matching version of kubectl. Otherwise, it defaults to execute the\n",
      "    current 1.11.7 version of kubectl.\n",
      "      o Updated extra Google Kubernetes Engine kubectl versions:\n",
      "        * kubectl.1.9 (patch 1.9.11)\n",
      "        * kubectl.1.10 (patch 1.10.13)\n",
      "        * kubectl.1.11 (patch 1.11.8)\n",
      "        * kubectl.1.12 (patch 1.12.6)\n",
      "        * kubectl.1.13 (patch 1.13.4)\n",
      "\n",
      "      o In June 2019, node auto-upgrade will be enabled by default for newly\n",
      "        created\n",
      "    clusters and node pools. To disable it, use the --no-enable-autoupgrade\n",
      "    flag.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "238.0.0 (2019-03-12)\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.84. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "\n",
      "      o Added a new field, 'main', to app.yaml. This allows users of the Go\n",
      "        second-generation runtimes to specify which package to build. See\n",
      "        https://cloud.google.com/appengine/docs/standard/go111/config/appref\n",
      "        for more information.\n",
      "\n",
      "  App Engine Flexible Environment\n",
      "      o Promoted network.session_affinity flag in yaml file to GA.\n",
      "\n",
      "  Cloud Access Context Manager\n",
      "      o Promoted the gcloud access-context-manager command group to GA\n",
      "      o Removed the unrestricted-services field from alpha and beta, always\n",
      "        set to default ''.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Promoted --optional-components of gcloud dataproc clusters to GA.\n",
      "\n",
      "  Cloud Functions\n",
      "      o Promoted max-instances and clear-max-instances flags of gcloud\n",
      "        functions deploy to beta.\n",
      "\n",
      "  Cloud Machine Learning Engine\n",
      "      o Promoted --machine-type flag of gcloud ml-engine versions create\n",
      "        command to GA.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Added the --enable-shielded-containers flag to gcloud beta container\n",
      "        clusters create.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "237.0.0 (2019-03-05)\n",
      "  Cloud SDK\n",
      "      o Fixed issue affecting users overwriting an existing Cloud SDK install\n",
      "        via the Windows installer, in which certain gcloud commands crashed\n",
      "        with a \"gcloud crashed (LayoutException): Multiple definitions for\n",
      "        release track\" error. This issue can be tracked at\n",
      "        <https://issuetracker.google.com/123390310>.\n",
      "\n",
      "  BigQuery\n",
      "      o Fixes bug in formatting pre-1900 timestamps.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Fixed a bug where several fields were hidden from gcloud dataproc\n",
      "        clusters\n",
      "    <import|export> in all release tracks. Note that these fields were always\n",
      "    available in gcloud dataproc clusters <create-from-file|describe>.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.37.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the --server-binding flag of gcloud compute sole-tenancy\n",
      "        node-templates create to beta.\n",
      "      o Reduced lower bound for data disk sizes from 10GB to 1GB for gcloud\n",
      "        compute instances create.\n",
      "      o Added resources-accelerator to gcloud beta compute commitments\n",
      "        create.\n",
      "      o Promoted all option of --ports flag for gcloud compute\n",
      "        forwarding-rules to GA.\n",
      "      o Added INSTANCE_TEMPLATE and VERSION_NAME columns to output of gcloud\n",
      "        compute instance-groups managed list-instances.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Updated Google Kubernetes Engine's kubectl from version 1.10.7 to\n",
      "        1.11.7.\n",
      "      o Added extra Google Kubernetes Engine kubectl versions:\n",
      "        * kubectl.1.9\n",
      "        * kubectl.1.10\n",
      "        * kubectl.1.11\n",
      "        * kubectl.1.12\n",
      "        * kubectl.1.13\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "236.0.0 (2019-02-26)\n",
      "  Cloud SDK\n",
      "      o Modified error handling for gcloud auth revoke when revoking a\n",
      "        service account token to print a friendly error message with more\n",
      "        detailed instructions on how to revoke the token.\n",
      "\n",
      "  Cloud Asset Inventory\n",
      "      o Added --folder flag to gcloud beta asset export command.\n",
      "\n",
      "  Cloud Dataproc\n",
      "      o Added --kerberos-root-principal-password-uri, --kerberos-kms-key, and\n",
      "        --kerberos-config-file flags to gcloud beta dataproc clusters create\n",
      "        and gcloud beta dataproc workflow-templates set-managed-cluster.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Released Cloud Firestore Emulator version 1.4.0:\n",
      "        * Added support for the debug(...) function in security rules.\n",
      "        * Simplified security rule evaluation and rule coverage reports.\n",
      "\n",
      "  Cloud Services\n",
      "      o Promoted services vpc-peerings command group to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Removed auto-create-routes column from default output of gcloud beta\n",
      "        compute networks peerings list.\n",
      "      o Promoted --enable-logging and --log-filter flags of gcloud compute\n",
      "        routers nats to beta.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "235.0.0 (2019-02-19)\n",
      "  App Engine\n",
      "      o Fixed bug where deleting a service at the same time as deleting an\n",
      "        app version in an unrelated service caused deletion of the version to\n",
      "        fail.\n",
      "      o Updated the Python SDK to version 1.9.83. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/python/release-notes\n",
      "      o Updated the Java SDK to version 1.9.72. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/java/release-notes\n",
      "\n",
      "  Cloud Composer\n",
      "      o Promoted --airflow-version and --image-version flags of gcloud\n",
      "        composer environments create to GA. These mutually exclusive flags can\n",
      "        be used to specify the airflow version or image version used within a\n",
      "        created environment.\n",
      "\n",
      "  Cloud IoT\n",
      "      o Promoted gcloud iot devices gateways command group to GA.\n",
      "\n",
      "  Cloud Key Management Service\n",
      "      o Exposed a new format of Cavium's attestation introduced in Cavium's\n",
      "        new 3.2-08 version.\n",
      "\n",
      "  Compute Engine\n",
      "      o Updated the gcloud beta compute start-iap-tunnel command for Cloud\n",
      "        IAP TCP Forwarding to listen on both IPv4 and IPv6 for localhost.\n",
      "      o Updated Windows PuTTY executables to 0.70.\n",
      "      o Promoted gcloud compute networks peerings list-routes command to\n",
      "        beta.\n",
      "      o Modified the output of gcloud beta compute networks peerings list to\n",
      "        include IMPORT_CUSTOM_ROUTES/EXPORT_CUSTOM_ROUTES columns.\n",
      "      o Promoted --resource-policies of gcloud compute disks create to beta.\n",
      "      o Promoted --enable-logging and --logging-sample-rate flags of gcloud\n",
      "        compute backend-services <create|update> to beta.\n",
      "      o Promoted --bandwidth flag of gcloud compute interconnects attachments\n",
      "        dedicated <create|update> to beta\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Added an ignore: option to the --robo-directives flag of gcloud beta\n",
      "        firebase test android run command. This option directs Robo to avoid\n",
      "        interactions with a user-defined UI element.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "234.0.0 (2019-02-12)\n",
      "  Cloud SDK\n",
      "      o Added the accessibility/screen_reader property. This property changes\n",
      "        some gcloud UX to make output more screen reader friendly. See gcloud\n",
      "        topic accessibility for more information.\n",
      "        * Accessibility support is still in the early stages, so please\n",
      "          report any issues that you would like fixed using gcloud feedback.\n",
      "\n",
      "  Cloud Asset Inventory\n",
      "      o Added gcloud beta asset command group to manage the Cloud Asset\n",
      "        Inventory.\n",
      "\n",
      "  Cloud Resource Manager\n",
      "      o Promoted resource-manager folders command group to beta\n",
      "\n",
      "  Cloud Services\n",
      "      o Added gcloud beta services vpc-peerings to support updating a\n",
      "        connection.\n",
      "\n",
      "  Cloud Source Repositories\n",
      "      o Promoted gcloud source project-configs command group to GA.\n",
      "      o Promoted gcloud source repos update to GA.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted gcloud beta compute networks subnets <get|set>-iam-policy\n",
      "        and <add|remove>-iam-policy-bindings to GA.\n",
      "      o Promoted the following commands to beta:\n",
      "        * gcloud compute disks <add|remove-resource-policies>\n",
      "        * gcloud compute resource-policies create-snapshot-schedule\n",
      "        * gcloud compute resource-policies delete\n",
      "        * gcloud compute resource-policies describe\n",
      "        * gcloud compute resource-policies list\n",
      "      o Promoted --service-label flag of gcloud compute forwarding-rules\n",
      "        create to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "233.0.0 (2019-02-05)\n",
      "  Cloud Dataflow\n",
      "      o Added numWorkers, network, subnetwork and workerMachineType flags to\n",
      "    'gcloud beta dataflow jobs run' command\n",
      "\n",
      "  Cloud Datalab\n",
      "      o Updated the datalab component to the 20190116 release. Released\n",
      "        changes are documented in its tracking issue at\n",
      "        https://github.com/googledatalab/datalab/issues/2114\n",
      "        (https://github.com/googledatalab/datalab/issues/2114).\n",
      "\n",
      "  Cloud Filestore\n",
      "      o Promoted gcloud filestore command group to GA.\n",
      "\n",
      "  Cloud Firestore Emulator\n",
      "      o Release Cloud Firestore Emulator version 1.3.0\n",
      "        * Added a clearDatabase RPC to delete all data in a database\n",
      "        * Added logging to assist with FIRESTORE_EMULATOR_HOST environment\n",
      "          variable\n",
      "        * The getDocument RPC now supports a read_time consistency selector\n",
      "        * Fixed bug related to rule evaluation callbacks\n",
      "\n",
      "  Cloud Machine Learning Engine\n",
      "      o Added support for custom server configuration to ml-engine jobs\n",
      "        submit training in beta. Added the following flags:\n",
      "        * --master-machine-type\n",
      "        * --master-accelerator\n",
      "        * --master-image-uri\n",
      "        * --worker-machine-type\n",
      "        * --worker-count\n",
      "        * --worker-accelerator\n",
      "        * --worker-image-uri\n",
      "        * --parameter-server-machine-type\n",
      "        * --parameter-server-count\n",
      "        * --parameter-server-accelerator\n",
      "        * --parameter-server-image-uri\n",
      "\n",
      "  Cloud Pub/Sub\n",
      "      o Promoted Snapshot & Seek features to GA. These features allow users\n",
      "        to create snapshots of subscription backlog state, and later restore\n",
      "        that state.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed gcloud sql connect whitelisting issues that resulted from\n",
      "        invalid datetime formatting.\n",
      "\n",
      "  Cloud Storage\n",
      "      o Updated gsutil component to 4.36.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted the --force-attach flag of compute instances attach-disk to\n",
      "        GA\n",
      "      o Added <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        gcloud beta compute networks subnets\n",
      "      o Promoted gcloud compute instances get-shielded-identity to beta.\n",
      "      o Promoted gcloud compute instance-groups managed update to GA together\n",
      "        with --health-check, --initial-delay and --clear-autohealing flags.\n",
      "      o Promoted --initial-delay and --health-check flags of gcloud compute\n",
      "        instance-groups managed create to GA.\n",
      "      o Enabled the use of multiple --network-interface flags with gcloud\n",
      "        compute <instances|instance-templates> create-with-container to support\n",
      "        using multiple network interfaces.\n",
      "      o Promoted gcloud compute instance-groups managed rolling-action\n",
      "        command group to GA.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "232.0.0 (2019-01-29)\n",
      "  Breaking Changes\n",
      "      o **(Kubernetes Engine)** Added a warning on cluster and node-pool\n",
      "        creation to notify users that modifications on the boot disks of node\n",
      "        VMs do not persist across node recreations and must be done using a\n",
      "        DaemonSet.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Rolled back fix to gcloud sql connect that seems to be causing\n",
      "        additional issues connecting.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        GA in the following command groups:\n",
      "        * gcloud compute disks\n",
      "        * gcloud compute images\n",
      "        * gcloud compute instance-templates\n",
      "        * gcloud compute snapshots\n",
      "      o Added '--enable-display-device' to gcloud beta compute instances\n",
      "        <create|update>\n",
      "      o Deprecated gcloud compute instance-groups managed set-autohealing\n",
      "        command. Use gcloud compute instance-groups managed update instead.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --database-encryption-key flag of gcloud container\n",
      "        clusters\n",
      "    create to beta. The flag enables support for encryption of Kubernetes\n",
      "    Secrets.\n",
      "      o Modified the --enable-stackdriver-kubernetes flag to be a hard\n",
      "        requirement\n",
      "    for --addons=CloudRun. The CloudRun-on-GKE add-on depends on Stackdriver\n",
      "    Kubernetes Monitoring to enrich Kubernetes metadata for logs and metrics.\n",
      "      o Add --max-pods-per-node for gcloud beta container clusters create.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "231.0.0 (2019-01-23)\n",
      "  Breaking Changes\n",
      "      o **(Cloud SQL)** Updated the error messaging associated with failed\n",
      "        long-running operations.\n",
      "\n",
      "  App Engine\n",
      "      o Updated the Python SDK to version 1.9.82. Please visit the following\n",
      "        release notes for details:\n",
      "        https://cloud.google.com/appengine/docs/standard/python/release-notes\n",
      "\n",
      "  BigQuery\n",
      "      o Added --ignore_unknown_values flag to bq mkdef command.\n",
      "      o Added support for BigQuery BI Engine reservations in bq cli.\n",
      "\n",
      "  Cloud Datastore Emulator\n",
      "      o Release Cloud Datastore Emulator version 2.1.0\n",
      "        * Implement export/import for emulator. For details, refer to\n",
      "          <https://cloud.google.com/datastore/docs/tools/emulator-export-import>.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Fixed the display of error codes in gcloud sql operations list.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted gcloud compute instance-groups managed update to beta\n",
      "        together with --health-check, --initial-delay and --clear-autohealing\n",
      "        flags.\n",
      "      o Promoted --hostname flag of gcloud compute instances create to GA.\n",
      "      o Added --physical-block-size flag to gcloud beta compute disks create.\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Added --num-flaky-test-attempts flag to gcloud beta firebase test\n",
      "        android run and gcloud beta firebase test ios run to rerun failed\n",
      "        executions multiple times.\n",
      "\n",
      "  Kubernetes Engine\n",
      "      o Promoted the --security-group flag of gcloud container clusters\n",
      "        create to\n",
      "    beta. The flag enables support for Google Groups in Kubernetes RBAC rules.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "230.0.0 (2019-01-15)\n",
      "  Breaking Changes\n",
      "      o **(App Engine)** Fixed a bug where symlinked directories were skipped\n",
      "        on source upload. Second Generation runtimes and source directories\n",
      "        using .gcloudignore now upload the contents of symlinked directories,\n",
      "        matching the behavior of First Generation runtimes. To explicitly skip\n",
      "        a symlinked directory, add it to .gcloudignore.\n",
      "      o **(Cloud Functions)** Fixed a bug where symlinked directories were\n",
      "        skipped on source upload. To explicitly skip a symlinked directory, add\n",
      "        it to .gcloudignore.\n",
      "      o **(Cloud SQL)** Made the flags --region, --gce-zone, and --zone\n",
      "        mutually exclusive for the command gcloud sql instances create.\n",
      "      o **(Cloud SQL)** Deprecated the creation of First Generation Cloud SQL\n",
      "        instances, adding a warning and confirmation prompt to gcloud sql\n",
      "        instances create.\n",
      "\n",
      "  Cloud Build\n",
      "      o Released cloud-build-local v0.5.0; see release notes:\n",
      "        <https://github.com/GoogleCloudPlatform/cloud-build-local/releases/tag/v0.5.0>.\n",
      "\n",
      "  Cloud SQL\n",
      "      o Added the flag --zone to gcloud sql instances create as an\n",
      "        alternative to --gce-zone, which is now deprecated.\n",
      "      o Deprecated First Generation Cloud SQL instances, adding warnings to\n",
      "        gcloud sql instances <describe|patch>.\n",
      "\n",
      "  Cloud Scheduler\n",
      "      o Added support for all of App Engine's regions to Cloud Scheduler.\n",
      "\n",
      "  Compute Engine\n",
      "      o Promoted <get|set>-iam-policy and <add|remove>-iam-policy-bindings to\n",
      "        GA in the following command groups:\n",
      "        * gcloud compute instances\n",
      "        * gcloud compute sole-tenancy node-templates\n",
      "        * gcloud compute sole-tenancy node-groups\n",
      "      o Promoted --boot flag of gcloud compute instances attach-disk to GA.\n",
      "      o Deprecated --auto-create-routes flag of gcloud alpha compute networks\n",
      "        peerings create in Beta.\n",
      "      o Promoted gcloud compute networks peerings update command to Beta.\n",
      "      o Promoted import-custom-routes and export-custom-routes flags to Beta\n",
      "        in gcloud compute networks peerings create command.\n",
      "      o Deprecated and renamed the following --shielded-vm- flags:\n",
      "        * --shielded-vm-secure-boot as --shielded-secure-boot\n",
      "        * --shielded-vm-vtpm as --shielded-vtpm\n",
      "        * --shielded-vm-integrity-monitoring as\n",
      "          --shielded-integrity-monitoring\n",
      "        * --shielded-vm-learn-integrity-policy as\n",
      "          --shielded-learn-integrity-policy\n",
      "\n",
      "  Firebase Test Lab\n",
      "      o Deprecated the --app-package and --test-package flags in gcloud\n",
      "        firebase test android run commands; the application and test package\n",
      "        names will be parsed from the APK manifest by default.\n",
      "      o Removed three robo test args that were deprecated 6+ months ago:\n",
      "        --max-steps, --max-depth, and --app-initial-activity.\n",
      "\n",
      "    Subscribe to these release notes at\n",
      "    https://groups.google.com/forum/#!forum/google-cloud-sdk-announce\n",
      "    (https://groups.google.com/forum/#!forum/google-cloud-sdk-announce).\n",
      "\n",
      "Do you want to continue (Y/n)?  \n",
      "#============================================================#\n",
      "#= Creating update staging area                             =#\n",
      "#============================================================#\n",
      "#= Uninstalling: BigQuery Command Line Tool                 =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud SDK Core Libraries                   =#\n",
      "#============================================================#\n",
      "#= Uninstalling: Cloud Storage Command Line Tool            =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Alpha Commands                      =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud Beta Commands                       =#\n",
      "#============================================================#\n",
      "#= Uninstalling: gcloud cli dependencies                    =#\n",
      "#============================================================#\n",
      "#= Installing: BigQuery Command Line Tool                   =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud SDK Core Libraries                     =#\n",
      "#============================================================#\n",
      "#= Installing: Cloud Storage Command Line Tool              =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Alpha Commands                        =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud Beta Commands                         =#\n",
      "#============================================================#\n",
      "#= Installing: gcloud cli dependencies                      =#\n",
      "#============================================================#\n",
      "#= Creating backup and activating new installation          =#\n",
      "#============================================================#\n",
      "\n",
      "Performing post processing steps...\n",
      "..........................................done.\n",
      "\n",
      "Update done!\n",
      "\n",
      "To revert your SDK to the previously installed version, you may run:\n",
      "  $ gcloud components update --version 229.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## ensure gcloud is up to date\n",
    "gcloud components update\n",
    "\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Specifying query to pull the data </h2>\n",
    "\n",
    "Let's pull out a few extra columns from the timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT\n",
      "  (tolls_amount + fare_amount) AS fare_amount,\n",
      "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
      "  HOUR(pickup_datetime) AS hourofday,\n",
      "  pickup_longitude AS pickuplon,\n",
      "  pickup_latitude AS pickuplat,\n",
      "  dropoff_longitude AS dropofflon,\n",
      "  dropoff_latitude AS dropofflat,\n",
      "  passenger_count*1.0 AS passengers,\n",
      "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
      "FROM\n",
      "  [nyc-tlc:yellow.trips]\n",
      "WHERE\n",
      "  trip_distance > 0\n",
      "  AND fare_amount >= 2.5\n",
      "  AND pickup_longitude > -78\n",
      "  AND pickup_longitude < -70\n",
      "  AND dropoff_longitude > -78\n",
      "  AND dropoff_longitude < -70\n",
      "  AND pickup_latitude > 37\n",
      "  AND pickup_latitude < 45\n",
      "  AND dropoff_latitude > 37\n",
      "  AND dropoff_latitude < 45\n",
      "  AND passenger_count > 0\n",
      "   AND ABS(HASH(pickup_datetime)) % 100 == 2\n"
     ]
    }
   ],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  if EVERY_N == None:\n",
    "    EVERY_N = 4 #use full dataset\n",
    "    \n",
    "  #select and pre-process fields\n",
    "  base_query = \"\"\"\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  DAYOFWEEK(pickup_datetime) AS dayofweek,\n",
    "  HOUR(pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  CONCAT(STRING(pickup_datetime), STRING(pickup_longitude), STRING(pickup_latitude), STRING(dropoff_latitude), STRING(dropoff_longitude)) AS key\n",
    "FROM\n",
    "  [nyc-tlc:yellow.trips]\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  \"\"\"\n",
    "  \n",
    "  #add subsampling criteria by modding with hashkey\n",
    "  if phase == 'train': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} < 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'valid': \n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 2\".format(base_query,EVERY_N)\n",
    "  elif phase == 'test':\n",
    "    query = \"{} AND ABS(HASH(pickup_datetime)) % {} == 3\".format(base_query,EVERY_N)\n",
    "  return query\n",
    "    \n",
    "print(create_query('valid', 100)) #example query using 1% of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the query above in https://bigquery.cloud.google.com/table/nyc-tlc:yellow.trips if you want to see what it does (ADD LIMIT 10 to the query!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Preprocessing Dataflow job from BigQuery </h2>\n",
    "\n",
    "This code reads from BigQuery and saves the data as-is on Google Cloud Storage.  We can do additional preprocessing and cleanup inside Dataflow, but then we'll have to remember to repeat that prepreprocessing during inference. It is better to use tf.transform which will do this book-keeping for you, or to do preprocessing within your TensorFlow model. We will look at this in future notebooks. For now, we are simply moving data from BigQuery to CSV using Dataflow.\n",
    "\n",
    "While we could read from BQ directly from TensorFlow (See: https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV.  Let's use Dataflow to do this at scale.\n",
    "\n",
    "Because we are running this on the Cloud, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. It will take several minutes for the preprocessing job to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if gsutil ls | grep -q gs://${BUCKET}/taxifare/ch4/taxi_preproc/; then\n",
    "  gsutil -m rm -rf gs://$BUCKET/taxifare/ch4/taxi_preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a function for preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
    "#     which each row is represented as a python dictionary\n",
    "# Returns:\n",
    "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
    "#     converted from int to string (e.g. 3 --> Tue)\n",
    "####\n",
    "def to_csv(rowdict):\n",
    "  days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
    "  CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
    "  rowdict['dayofweek'] = days[rowdict['dayofweek']]\n",
    "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
    "  return rowstring\n",
    "\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
    "#     Larger values will yield smaller sample\n",
    "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
    "#     locally or on Google Cloud respectively. \n",
    "# Side-effects:\n",
    "#   -Creates and executes dataflow pipeline. \n",
    "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "####\n",
    "def preprocess(EVERY_N, RUNNER):\n",
    "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "  print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "  OUTPUT_DIR = 'gs://{0}/taxifare/ch4/taxi_preproc/'.format(BUCKET)\n",
    "\n",
    "  #dictionary of pipeline options\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'runner': RUNNER\n",
    "  }\n",
    "  #instantiate PipelineOptions object using options dictionary\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  #instantantiate Pipeline object using PipelineOptions\n",
    "  with beam.Pipeline(options=opts) as p:\n",
    "      for phase in ['train', 'valid']:\n",
    "        query = create_query(phase, EVERY_N) \n",
    "        outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase))\n",
    "        (\n",
    "          p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query=query))\n",
    "            | 'tocsv_{}'.format(phase) >> beam.Map(to_csv)\n",
    "            | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile))\n",
    "        )\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run pipeline locally. This takes upto <b>5 minutes</b>.  You will see a message \"Done\" when it is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-190418-062607 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:365: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  pipeline.replace_all(_get_transform_overrides(pipeline.options))\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/oauth2client/contrib/gce.py:99: UserWarning: You have requested explicit scopes to be used with a GCE service account.\n",
      "Using this argument will have no effect on the actual scopes for tokens\n",
      "requested. These scopes are set at VM instance creation time and\n",
      "can't be overridden in the request.\n",
      "\n",
      "  warnings.warn(_SCOPES_WARNING)\n",
      "WARNING:root:Dataset qwiklabs-gcp-631b9e1cceeff549:temp_dataset_a12ab6a8dbd14a55a451872252518884 does not exist so we will create it as temporary with location=US\n",
      "WARNING:root:Dataset qwiklabs-gcp-631b9e1cceeff549:temp_dataset_18b4c14621c84f1e86751fd2a0486b82 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(50*10000, 'DirectRunner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/train.csv-00000-of-00005\n",
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/train.csv-00001-of-00005\n",
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/train.csv-00002-of-00005\n",
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/train.csv-00003-of-00005\n",
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/train.csv-00004-of-00005\n",
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/valid.csv-00000-of-00002\n",
      "gs://qwiklabs-gcp-631b9e1cceeff549/taxifare/ch4/taxi_preproc/valid.csv-00001-of-00002\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Beam pipeline on Cloud Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipeline on cloud on a larger sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if gsutil ls | grep -q gs://${BUCKET}/taxifare/ch4/taxi_preproc/; then\n",
    "  gsutil -m rm -rf gs://$BUCKET/taxifare/ch4/taxi_preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step will take <b>15-20 minutes.</b> Monitor job progress on the [Cloud Console, in the Dataflow](https://console.cloud.google.com/dataflow) section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-190418-062651 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:800: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  options = pbegin.pipeline.options.view_as(DebugOptions)\n"
     ]
    },
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(pytz 2019.1 (/usr/local/envs/py2env/lib/python2.7/site-packages), Requirement.parse('pytz<=2018.4,>=2018.3'), set(['apache-beam']))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b4775e416971>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DataflowRunner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#change first arg to None to preprocess full dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0ab357cc98ce>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(EVERY_N, RUNNER)\u001b[0m\n\u001b[1;32m     50\u001b[0m           \u001b[0mp\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m'read_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigQuerySource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;34m|\u001b[0m \u001b[0;34m'tocsv_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;34m|\u001b[0m \u001b[0;34m'write_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWriteToText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.pyc\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 394\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    498\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;34m\"\"\"Creates job description. May stage and/or submit for remote execution.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;31m# Stage and submit the job when necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;31m# Stage other resources for the SDK harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stage_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     job.proto.environment = Environment(\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36m_stage_resources\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mtemp_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         staging_location=google_cloud_options.staging_location)\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36mstage_job_resources\u001b[0;34m(self, options, build_setup_args, temp_dir, populate_requirements_cache, staging_location)\u001b[0m\n\u001b[1;32m    226\u001b[0m         resources.extend(\n\u001b[1;32m    227\u001b[0m             self._stage_beam_sdk(sdk_remote_location, staging_location,\n\u001b[0;32m--> 228\u001b[0;31m                                  temp_dir))\n\u001b[0m\u001b[1;32m    229\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0msetup_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# Use the SDK that's built into the container, rather than re-staging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36m_stage_beam_sdk\u001b[0;34m(self, sdk_remote_location, staging_location, temp_dir)\u001b[0m\n\u001b[1;32m    479\u001b[0m       \"\"\"\n\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msdk_remote_location\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pypi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m       \u001b[0msdk_local_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_pypi_sdk_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m       \u001b[0msdk_sources_staged_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m           \u001b[0m_desired_sdk_filename_in_staging_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdk_local_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/portability/stager.pyc\u001b[0m in \u001b[0;36m_download_pypi_sdk_package\u001b[0;34m(temp_dir, fetch_binary, language_version_tag, language_implementation_tag, abi_tag, platform_tag)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0mpackage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sdk_package_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m       \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       raise RuntimeError('Please set --sdk_location command-line option '\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected string, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \"\"\"\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/pkg_resources/__init__.pyc\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (pytz 2019.1 (/usr/local/envs/py2env/lib/python2.7/site-packages), Requirement.parse('pytz<=2018.4,>=2018.3'), set(['apache-beam']))"
     ]
    }
   ],
   "source": [
    "preprocess(50*100, 'DataflowRunner') \n",
    "#change first arg to None to preprocess full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job completes, observe the files created in Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls -l gs://$BUCKET/taxifare/ch4/taxi_preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#print first 10 lines of first shard of train.csv\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Develop model with new inputs\n",
    "\n",
    "Download the first shard of the preprocessed data to enable local development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d sample ]; then\n",
    "  rm -rf sample\n",
    "fi\n",
    "mkdir sample\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/train.csv-00000-of-*\" > sample/train.csv\n",
    "gsutil cat \"gs://$BUCKET/taxifare/ch4/taxi_preproc/valid.csv-00000-of-*\" > sample/valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two new inputs in the INPUT_COLUMNS, three engineered features, and the estimator involves bucketization and feature crosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -A 20 \"INPUT_COLUMNS =\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -A 50 \"build_estimator\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -A 15 \"add_engineered(\" taxifare/trainer/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out the new model on the local sample (this takes <b>5 minutes</b>) to make sure it works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/taxifare\n",
    "python -m trainer.task \\\n",
    "  --train_data_paths=${PWD}/sample/train.csv \\\n",
    "  --eval_data_paths=${PWD}/sample/valid.csv  \\\n",
    "  --output_dir=${PWD}/taxi_trained \\\n",
    "  --train_steps=10 \\\n",
    "  --job-dir=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls taxi_trained/export/exporter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use ```saved_model_cli``` to look at the exported signature. Note that the model doesn't need any of the engineered features as inputs. It will compute latdiff, londiff, euclidean from the provided inputs, thanks to the ```add_engineered``` call in the serving_input_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/taxi_trained/export/exporter | tail -1)\n",
    "saved_model_cli show --dir ${PWD}/taxi_trained/export/exporter/${model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/test.json\n",
    "{\"dayofweek\": \"Sun\", \"hourofday\": 17, \"pickuplon\": -73.885262, \"pickuplat\": 40.773008, \"dropofflon\": -73.987232, \"dropofflat\": 40.732403, \"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/taxi_trained/export/exporter)\n",
    "gcloud ml-engine local predict \\\n",
    "  --model-dir=${PWD}/taxi_trained/export/exporter/${model_dir} \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train on cloud\n",
    "\n",
    "This will take <b> 10-15 minutes </b> even though the prompt immediately returns after the job is submitted. Monitor job progress on the [Cloud Console, in the ML Engine](https://console.cloud.google.com/mlengine) section and wait for the training job to complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/taxifare/ch4/taxi_trained\n",
    "JOBNAME=lab4a_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "  --region=$REGION \\\n",
    "  --module-name=trainer.task \\\n",
    "  --package-path=${PWD}/taxifare/trainer \\\n",
    "  --job-dir=$OUTDIR \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --scale-tier=BASIC \\\n",
    "  --runtime-version=$TFVERSION \\\n",
    "  -- \\\n",
    "  --train_data_paths=\"gs://$BUCKET/taxifare/ch4/taxi_preproc/train*\" \\\n",
    "  --eval_data_paths=\"gs://${BUCKET}/taxifare/ch4/taxi_preproc/valid*\"  \\\n",
    "  --train_steps=5000 \\\n",
    "  --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE is now 8.33249, an improvement over the 9.3 that we were getting ... of course, we won't know until we train/validate on a larger dataset. Still, this is promising. But before we do that, let's do hyper-parameter tuning.\n",
    "\n",
    "<b>Use the Cloud Console link to monitor the job and do NOT proceed until the job is done.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1)\n",
    "saved_model_cli show --dir ${model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "model_dir=$(gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1)\n",
    "gcloud ml-engine local predict \\\n",
    "  --model-dir=${model_dir} \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: deploy model to cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"feateng\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/taxifare/ch4/taxi_trained/export/exporter | tail -1)\n",
    "echo \"Run these commands one-by-one (the very first time, you'll create a model and then create a version)\"\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version $TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ml-engine predict --model=feateng --version=v1 --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6. Hyper-parameter tune </h2>\n",
    "\n",
    "Look at <a href=\"hyperparam.ipynb\">hyper-parameter tuning notebook</a> to decide what parameters to use for model. Based on that run, I ended up choosing:\n",
    "<ol>\n",
    "<li> train_batch_size: 512 </li>\n",
    "<li> nbuckets: 16 </li>\n",
    "<li> hidden_units: \"64 64 64 8\" </li>    \n",
    "</ol>\n",
    "\n",
    "This gives an RMSE of 5, a considerable improvement from the 8.3 we were getting earlier ... Let's try this over a larger dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Run Cloud training on 2 million row dataset\n",
    "\n",
    "This run uses as input 2 million rows and takes ~20 minutes with 10 workers (STANDARD_1 pricing tier). The model is exactly the same as above. The only changes are to the input (to use the larger dataset) and to the Cloud MLE tier (to use STANDARD_1 instead of BASIC -- STANDARD_1 is approximately 10x more powerful than BASIC). Because the Dataflow preprocessing takes about 15 minutes, we train here using CSV files in a public bucket.\n",
    "\n",
    "When doing distributed training, use train_steps instead of num_epochs. The distributed workers don't know how many rows there are, but we can calculate train_steps = num_rows \\* num_epochs / train_batch_size. In this case, we have 2141023 * 100 / 512 = 418168 train steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "WARNING -- this uses significant resources and is optional. Remove this line to run the block.\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/taxifare/feateng2m\n",
    "JOBNAME=lab4a_$(date -u +%y%m%d_%H%M%S)\n",
    "TIER=STANDARD_1 \n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/taxifare/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=$TIER \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/valid*\"  \\\n",
    "   --output_dir=$OUTDIR \\\n",
    "   --train_steps=418168 \\\n",
    "   --train_batch_size=512 --nbuckets=16 --hidden_units=\"64 64 64 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "OUTDIR='gs://{0}/taxifare/feateng2m'.format(BUCKET)\n",
    "print(OUTDIR)\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_df = TensorBoard.list()\n",
    "if not pids_df.empty:\n",
    "    for pid in pids_df['pid']:\n",
    "        TensorBoard().stop(pid)\n",
    "        print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE after training on the 2-million-row dataset is \\$3.03.  This graph shows the improvements so far ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({'Lab' : pd.Series(['1a', '2-3', '4a', '4b', '4c']),\n",
    "              'Method' : pd.Series(['Heuristic Benchmark', 'tf.learn', '+Feature Eng.', '+ Hyperparam', '+ 2m rows']),\n",
    "              'RMSE': pd.Series([8.026, 9.4, 8.3, 5.0, 3.03]) })\n",
    "\n",
    "ax = sns.barplot(data = df, x = 'Method', y = 'RMSE')\n",
    "ax.set_ylabel('RMSE (dollars)')\n",
    "ax.set_xlabel('Labs/Methods')\n",
    "plt.plot(np.linspace(-20, 120, 1000), [5] * 1000, 'b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil -m mv gs://${BUCKET}/taxifare/ch4/ gs://${BUCKET}/taxifare/ch4_1m/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2016 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
